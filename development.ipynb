{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "tavily_api_key = os.environ.get(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "import chromadb\n",
    "\n",
    "run_local = \"No\"\n",
    "# Embed and index\n",
    "if run_local == \"Yes\":\n",
    "    embedding = GPT4AllEmbeddings()\n",
    "else:\n",
    "    embedding = GPT4AllEmbeddings()\n",
    "    #embedding=OpenAIEmbeddings()\n",
    "\n",
    "if os.path.isdir('knowledge_base'):\n",
    "    persistent_client = chromadb.PersistentClient(path=\"./knowledge_base\")\n",
    "    vectorstore = Chroma(\n",
    "       client=persistent_client,\n",
    "       embedding_function=embedding,\n",
    "       collection_name= \"knowledge_base\"\n",
    "       )\n",
    "else:\n",
    "    # Load\n",
    "    loader = DirectoryLoader('.', glob=\"./papers/*.pdf\", loader_cls=PyPDFLoader)\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Split\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=500, chunk_overlap=100\n",
    "    )\n",
    "    all_splits = text_splitter.split_documents(docs)\n",
    "    # Index\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=all_splits,\n",
    "        collection_name=\"knowledge_base\",\n",
    "        embedding=embedding,\n",
    "        persist_directory=\"./knowledge_base\",\n",
    "    )\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Dict, TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        keys: A dictionary where each key is a string.\n",
    "    \"\"\"\n",
    "\n",
    "    keys: Dict[str, any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import operator\n",
    "import pprint\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "### Nodes ###\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    local = state_dict[\"local\"]\n",
    "    documents = retriever.get_relevant_documents(question)\n",
    "    return {\"keys\": {\"documents\": documents, \"local\": local, \"question\": question}}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    local = state_dict[\"local\"]\n",
    "\n",
    "    # Prompt\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # LLM\n",
    "    if local == \"Yes\":\n",
    "        llm = ChatOpenAI(openai_api_base=\"http://127.0.0.1:8081\", openai_api_key='na', model='Llama2')\n",
    "    else:\n",
    "        llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "    # Post-processing\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    str_docs = 'document:\\n'.join([doc.page_content for doc in documents])\n",
    "    print(str_docs)\n",
    "    generation = rag_chain.invoke({\"context\": str_docs, \"question\": question})\n",
    "    return {\n",
    "        \"keys\": {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "    }\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    local = state_dict[\"local\"]\n",
    "\n",
    "    # LLM\n",
    "    if local == \"Yes\":\n",
    "        llm = ChatOpenAI(openai_api_base=\"http://127.0.0.1:8081\", openai_api_key='na', model='Llama2')\n",
    "    else:\n",
    "        llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "    # Data model\n",
    "    #class grade(BaseModel):\n",
    "    #    \"\"\"Binary score for relevance check.\"\"\"\n",
    "\n",
    "    #    score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # Set up a parser + inject instructions into the prompt template.\n",
    "    #parser = PydanticOutputParser(pydantic_object=grade)\n",
    "\n",
    "    from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "    #parser = JsonOutputParser(pydantic_object=grade)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is an example:\n",
    "        [INST]Banks are very important institutions.[/INST]\n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keywords related to the user question, grade it as yes. \\n\n",
    "        It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "        Only reply 'yes' or 'no'. Do not include any other words or phrases in your reply.\\n\n",
    "        Here are some examples example:\n",
    "        \\n\n",
    "        retrieved document: Banks are very important institutions.\n",
    "        \\n\n",
    "        user question: what are banks?\n",
    "        \\n\n",
    "        <s>Answer: yes </s>\n",
    "        \\n\n",
    "        retrieved document: students are very studious.\n",
    "        \\n\n",
    "        user question: what are banks?\n",
    "        \\n\n",
    "        <s>Answer: no </s>\\n\n",
    "        retrieved document: \\n\\n {context} \\n\\n\n",
    "        user question: {question} \\n\n",
    "        Answer: \n",
    "        \"\"\",\n",
    "        input_variables=[\"query\"],\n",
    "        #partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm | StrOutputParser() #parser\n",
    "\n",
    "    # Score\n",
    "    filtered_docs = []\n",
    "    search = \"No\"  # Default do not opt for web search to supplement retrieval\n",
    "    for d in documents:\n",
    "        try:\n",
    "            score = chain.invoke(\n",
    "                {\n",
    "                    \"question\": question,\n",
    "                    \"context\": d.page_content,\n",
    "                    #\"format_instructions\": parser.get_format_instructions(),\n",
    "                }\n",
    "            )\n",
    "            grade = 'yes' if 'yes' in score.lower() else 'no'\n",
    "            print(score)\n",
    "        except:\n",
    "            grade = \"no\"\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            search = \"Yes\"  # Perform web search\n",
    "            continue\n",
    "\n",
    "    return {\n",
    "        \"keys\": {\n",
    "            \"documents\": filtered_docs,\n",
    "            \"question\": question,\n",
    "            \"local\": local,\n",
    "            \"run_web_search\": search,\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    local = state_dict[\"local\"]\n",
    "\n",
    "    # Create a prompt template with format instructions and the query\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are generating questions that is well optimized for retrieval. \\n \n",
    "        Look at the input and try to reason about the underlying sematic intent / meaning. \\n \n",
    "        Here is the initial question:\n",
    "        \\n ------- \\n\n",
    "        {question} \n",
    "        \\n ------- \\n\n",
    "        Provide an improved question without any premable, only respond with the updated question. Do not include any other notes, comments etc. \"\"\",\n",
    "        input_variables=[\"question\"],\n",
    "    )\n",
    "\n",
    "    # Grader\n",
    "    # LLM\n",
    "    if local == \"Yes\":\n",
    "        llm = ChatOpenAI(openai_api_base=\"http://127.0.0.1:8081\", openai_api_key='na', model='Llama2')\n",
    "    else:\n",
    "        llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "    # Prompt\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    better_question = chain.invoke({\"question\": question})\n",
    "\n",
    "    #print(better_question)\n",
    "\n",
    "    return {\n",
    "        \"keys\": {\"documents\": documents, \"question\": better_question, \"local\": local}\n",
    "    }\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question using Tavily API.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Web results appended to documents.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    local = state_dict[\"local\"]\n",
    "\n",
    "    tool = TavilySearchResults()\n",
    "    docs = tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "\n",
    "    return {\"keys\": {\"documents\": documents, \"local\": local, \"question\": question}}\n",
    "\n",
    "\n",
    "### Edges\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer or re-generate a question for web search.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current state of the agent, including all keys.\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---DECIDE TO GENERATE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    filtered_documents = state_dict[\"documents\"]\n",
    "    search = state_dict[\"run_web_search\"]\n",
    "\n",
    "    if search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"---DECISION: TRANSFORM QUERY and RUN WEB SEARCH---\")\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
    "workflow.add_node(\"web_search\", web_search)  # web search\n",
    "\n",
    "# Build graph\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"web_search\")\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "---CHECK RELEVANCE---\n",
      "no\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "No\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "no\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "No\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---DECIDE TO GENERATE---\n",
      "---DECISION: TRANSFORM QUERY and RUN WEB SEARCH---\n",
      "---TRANSFORM QUERY---\n",
      "---WEB SEARCH---\n",
      "---GENERATE---\n",
      "USCIS Form I-290B OMB No. 1615-0095 Expires 05/31/2020 To be completed by an attorney or accredited representative (if any). Select this box if Form G-28 is attached. Attorney State Bar Number (if applicable) Attorney or Accredited Representative USCIS Online Account Number (if any)\n",
      "Use this form to file: An appeal with the Administrative Appeals Office (AAO); A motion with the USCIS office that issued the latest decision in your case (including a field office, service center, or the AAO); or\n",
      "Applicants with grounds to overturn a petition must submit Form I-290B to request a review by the USCIS. To be exact, an application will be filed and review by the Administrative Appeals Office (AAO) or a local branch of the USCIS including service centers and field offices. To be eligible, applications must be categorized as one of the following:\n",
      "1. Review the Decision Thoroughly examine the USCIS decision letter to understand the grounds for denial or rejection, the deadline for filing an appeal, and the appropriate appealable decisions. 2. Complete the I-290B Form Fill out the I-290B form accurately, providing detailed explanations and supporting documentation.\n",
      "Form I-290B can be completed through the online portal Pay.gov. The form should be filed within 30 days of the order or within 33 days if the order has been issued through the mail. Instructions on where to send the appeal will be given on the denial notice itself. Visit the Pay.gov site and search for Form I-290B.\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\n",
    "    \"keys\": {\n",
    "        \"question\": \"how do I prepare the form I-290B?\",\n",
    "        \"local\": run_local,\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint.pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint.pprint(value['keys']['generation'])\n",
    "\"\"\"\n",
    "result = app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('To complete Form I-290B, thoroughly review the USCIS decision letter to '\n",
      " 'understand the grounds for denial or rejection and the deadline for filing '\n",
      " 'an appeal. Then, accurately fill out the I-290B form with detailed '\n",
      " 'explanations and supporting documentation. The form can be completed online '\n",
      " 'through the Pay.gov portal and should be filed within 30 days of the order '\n",
      " 'or within 33 days if the order was issued through the mail.')\n",
      "[Document(page_content='USCIS Form I-290B OMB No. 1615-0095 Expires 05/31/2020 To be completed by an attorney or accredited representative (if any). Select this box if Form G-28 is attached. Attorney State Bar Number (if applicable) Attorney or Accredited Representative USCIS Online Account Number (if any)\\nUse this form to file: An appeal with the Administrative Appeals Office (AAO); A motion with the USCIS office that issued the latest decision in your case (including a field office, service center, or the AAO); or\\nApplicants with grounds to overturn a petition must submit Form I-290B to request a review by the USCIS. To be exact, an application will be filed and review by the Administrative Appeals Office (AAO) or a local branch of the USCIS including service centers and field offices. To be eligible, applications must be categorized as one of the following:\\n1. Review the Decision Thoroughly examine the USCIS decision letter to understand the grounds for denial or rejection, the deadline for filing an appeal, and the appropriate appealable decisions. 2. Complete the I-290B Form Fill out the I-290B form accurately, providing detailed explanations and supporting documentation.\\nForm I-290B can be completed through the online portal Pay.gov. The form should be filed within 30 days of the order or within 33 days if the order has been issued through the mail. Instructions on where to send the appeal will be given on the denial notice itself. Visit the Pay.gov site and search for Form I-290B.')]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(result['keys']['generation'])\n",
    "pprint.pprint(result['keys']['documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = result['keys']['documents'][0]\n",
    "doc.metadata"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
